{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> GroupByKey Vs ReduceByKey </h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "We have a students data in the file **students_data.txt** which has the 9 different columns. First one is **Roll No**, **Section**, **Name of Student**, **City**, and the last five columns are marks in 5 different subjects. The data of each student is in different row separated by space.\n",
    "\n",
    "---\n",
    "\n",
    "![](images/data_1.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### `Create the rdd of the file - students_data.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, We will split each line into a list of words using Map. Let's see how to do this with the help of map transformation in the below cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Find out the number of students in each section?`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now, we will create another paired RDD where key will be the section of the student and marks of students as the values.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `TRANSFORMATION - GROUPBYKEY`\n",
    "\n",
    "---\n",
    "\n",
    "It receives key-value pairs (K, V) as an input, group the values based on key and generates a dataset of (K, Iterable) pairs as an output.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![](images/group.png)\n",
    "\n",
    "---\n",
    "\n",
    "MapValues is applicable only for pair RDDs. As its name indicates, this transformation only operates on the values of the pair RDDs instead of operating on the whole tuple.\n",
    "\n",
    "More about MapValues: \n",
    "\n",
    " - http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mapvalues#pyspark.RDD.mapValues\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### `TRANSFORMATION -  REDUCEBYKEY`\n",
    "\n",
    "---\n",
    "\n",
    "It uses associative reduce function, where it merges value of each key. It can be used with Rdd only in key value pair.  It merges data locally using associative function for optimized data shuffling. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](images/reduce.png)\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "The below image shows how the reduce function works:\n",
    "\n",
    "---\n",
    "\n",
    "![](images/reduce.svg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Comparison between GroupByKey and ReduceByKey`\n",
    "\n",
    "---\n",
    "\n",
    "We will create a sample data of 20 million data points and find out the sum of each key using both `groupByKey` and `reduceByKey`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, the above list will look something like this.\n",
    "\n",
    "---\n",
    "\n",
    " [ (\"A\": 1),\n",
    " \n",
    "   (\"B\": 2),\n",
    " \n",
    "   (\"C\": 1),\n",
    "   \n",
    "   .\n",
    "   \n",
    "   .\n",
    "   \n",
    "   .\n",
    "   \n",
    "   (\"A\": 4)\n",
    "]\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "**Create RDD of the collection**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `GroupByKey`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![](images/groupByKey.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the above image, we can see that total 27 MB of data was shuffled across the partitions. Now, let's do the same task using the `reduceByKey` and see if there is any difference in the performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `ReduceByKey`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![](images/reduceByKey.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using `reduceByKey`, only 2.3 KB of data was shuffled. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
